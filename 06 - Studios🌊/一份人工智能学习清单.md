---
creation date: 2023-09-26 20:31 
---
#🌲长青 #AI #人工智能

引用自[一份人工智能学习清单 - web3defi-web3贴吧](https://web3defi.org/d/981-%E4%B8%80%E4%BB%BD%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E6%B8%85%E5%8D%95)

1. 阅读 Andrej Karpathy 的所有博客文章  
2. 阅读 Chris Olah 的所有博客文章  
2.5 阅读你感兴趣的 Distill 上的任何帖子。或者看下我列出的帖子([Qreydanus.qithub.io](https://qreydanus.qithub.io/))  
3. 也许 - 参加像 Andrew Ng 的 Coursera 课程这样的在线课程  
4. 绝对 - 使用 Jupyter Notebook、NumPy 和 PyTorch 编写简单的个人项目。当你完成它们时 a) 发布良好的、记录良好的代码（参见我的 github） b) 写一篇关于你所做的事情的简短博客文章（参见我的博客）  
5. 下载Arx应用程序，浏览 Arxiv（机器学习预印本的在线存储库）上的论文。每天左右在通勤途中检查一下。遵循 cs.LG、cs.NE 和 stat.ML 标签。另外，请为以下作者加注星标：Yoshua Bengio、Yann LeCunn、Geoffery Hinton、Jason Yosinski、David Duvenaud、Andrej Karpathy、Pieter Abbeel、Quoc Lee、Alex Graves、Koray Kavukcuoglu、Gabor Melis、Oriol Vinyals、Jasch Sohl-Dickstein、Ian Goodfellow 和Adam Santoro。如果及时了解他们上传的论文，并浏览我提到的三个类别中论文的标题/摘要，就可以很快对 SOTA 研究有一个有效的了解。或者：开始每天浏览 Arxiv Sanity Preserver 的“热门炒作”和“最近热门”选项卡。  
6. 当/如果你开始在达特茅斯进行研究时，请确保涉及深度学习的一些元素。  
7. 如果可以在 PyTorch 和 TensorFlow 之间进行选择，请选择 PyTorch。你会终生感激这个决定。  
8. 值得阅读的热门论文：AlexNet 论文、Alex Graves“生成序列”论文、Jason Yosinski（他是一位优秀作者）的任何论文、神经图灵机论文、DeepMind Atari 论文，也许还有 Goodfellow 的 GAN 论文，尽管我还没有读过。如果可以的话，远离 GAN。  
9. 在 ML 阶段，简单问题 + 超简单实验 » 大型、多 GPU 的工作。有很多好的研究（例如，到目前为止我几乎所有的工作）都可以在一台像样的 MacBook 上完成。  
10. 不要被这份清单淹没。你可能会找到更适合自己的道路。我能给出的最好建议就是重复Richard Feynman的建议：“以尽可能无纪律、无礼和原创的方式努力学习你最感兴趣的东西。”









